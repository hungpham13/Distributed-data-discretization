{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import process_time\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# settings to display all columns\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metrics\n",
    "def precision_0_recall_1_inverse_weighted_fbeta(y_true, y_pred, beta=2.0):\n",
    "    precisions, recalls, fbeta_scores, supports = precision_recall_fscore_support(y_true, y_pred, beta=beta, average=None)\n",
    "\n",
    "    precision_0 = round(precisions[0], 4)\n",
    "    recall_1 = round(recalls[1], 4)\n",
    "    ratio_0, ratio_1 = supports / sum(supports)\n",
    "    inverse_weighted_fbeta_score = round(fbeta_scores[0]*ratio_1 + fbeta_scores[1]*ratio_0, 4)\n",
    "    \n",
    "    return precision_0, recall_1, inverse_weighted_fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cut points of EWB for histogram data\n",
    "def equal_width_cut_points(lower_bound, upper_bound, n_bins, hist_data):\n",
    "    for i in range(len(hist_data)):\n",
    "        if hist_data[i] != 0:\n",
    "            min_value = i + lower_bound\n",
    "            break\n",
    "    \n",
    "    for i in range(len(hist_data) - 1, -1, -1):\n",
    "        if hist_data[i] != 0:\n",
    "            max_value = i + lower_bound\n",
    "            break\n",
    "    \n",
    "    bin_width = (max_value - min_value) / n_bins\n",
    "    cut_points = [round(min_value + i * bin_width) for i in range(0, n_bins + 1)]\n",
    "    \n",
    "    if lower_bound not in cut_points:\n",
    "        cut_points.insert(0, lower_bound)\n",
    "    if upper_bound not in cut_points:\n",
    "        cut_points.append(upper_bound)\n",
    "    \n",
    "    return cut_points\n",
    "\n",
    "def equal_width_cut_points_naive(lower_bound, upper_bound, n_bins):    \n",
    "    bin_width = (upper_bound - lower_bound) / n_bins\n",
    "    cut_points = [round(lower_bound + i * bin_width) for i in range(0, n_bins + 1)]\n",
    "    \n",
    "    if lower_bound not in cut_points:\n",
    "        cut_points.insert(0, lower_bound)\n",
    "    if upper_bound not in cut_points:\n",
    "        cut_points.append(upper_bound)\n",
    "    \n",
    "    return cut_points\n",
    "\n",
    "# Get cut points of EFB for histogram data\n",
    "def equal_freq_cut_points(lower_bound, upper_bound, n_bins, hist_data):\n",
    "    total_count = sum(hist_data)\n",
    "    bin_size = total_count / n_bins\n",
    "    cumulative_count = 0\n",
    "    cut_points = []\n",
    "    for i in range(len(hist_data)):\n",
    "        cumulative_count += hist_data[i]\n",
    "        if cumulative_count >= bin_size:\n",
    "            cut_point = i + 1 + lower_bound\n",
    "            cut_points.append(cut_point)\n",
    "            cumulative_count = 0\n",
    "        if len(cut_points) == n_bins - 1:\n",
    "            break\n",
    "    \n",
    "    if lower_bound not in cut_points:\n",
    "        cut_points.insert(0, lower_bound)\n",
    "    if upper_bound not in cut_points:\n",
    "        cut_points.append(upper_bound)\n",
    "            \n",
    "    return cut_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "def load_data(train_data_path, test_data_dir):\n",
    "    # Training data\n",
    "    train_data = np.load(train_data_path)\n",
    "    \n",
    "    # Testing data\n",
    "    test_data_paths = glob(f\"{test_data_dir}/*.npy\")\n",
    "    test_data_all = [np.load(test_data_path) for test_data_path in test_data_paths]\n",
    "        \n",
    "    return train_data, test_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "def preprocess_data(train_data, test_data_all):\n",
    "    # Training features and labels\n",
    "    X_train = train_data[:, :-1]\n",
    "    X_train = X_train / sum(X_train[0])\n",
    "    y_train = train_data[1:, -1]\n",
    "\n",
    "    # Testing features and labels\n",
    "    X_test_all = [test_data[:, :-1] / sum(test_data[0, :-1]) for test_data in test_data_all]\n",
    "    y_test_all = [test_data[1:, -1] for test_data in test_data_all]\n",
    "\n",
    "    return X_train, y_train, X_test_all, y_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_objectives(X_test, y_test, final_bin_edges, alphas, epsilon=1e-8):\n",
    "    num_days_test = X_test.shape[0]\n",
    "    psi_0_test = psi_1_test = 0\n",
    "\n",
    "    for i in range(num_days_test - 1):\n",
    "        hist_1 = []\n",
    "        for j in range(len(final_bin_edges) - 1):\n",
    "            hist_1.append(np.sum(X_test[i, final_bin_edges[j] - 300: final_bin_edges[j + 1] - 300]))\n",
    "        hist_1 = np.array(hist_1)\n",
    "\n",
    "        hist_2 = []\n",
    "        for j in range(len(final_bin_edges) - 1):\n",
    "            hist_2.append(np.sum(X_test[i + 1, final_bin_edges[j] - 300: final_bin_edges[j + 1] - 300]))\n",
    "        hist_2 = np.array(hist_2)\n",
    "\n",
    "        psis = (hist_1 - hist_2) * np.log((hist_1 + epsilon) / (hist_2 + epsilon))\n",
    "        psi = np.sum(psis)\n",
    "\n",
    "        if y_test[i] == 0:\n",
    "            psi_0_test += psi\n",
    "        else:\n",
    "            psi_1_test += psi\n",
    "\n",
    "    psi_0_test = psi_0_test / np.sum(1 - y_test)\n",
    "    psi_1_test = psi_1_test / np.sum(y_test)\n",
    "    all_total_objective_test = {}\n",
    "\n",
    "    for alpha in alphas:\n",
    "        total_objective_test = alpha*psi_1_test - (1 - alpha)*psi_0_test\n",
    "        all_total_objective_test[alpha] = total_objective_test\n",
    "\n",
    "    return psi_0_test, psi_1_test, all_total_objective_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve\n",
    "def solve(train_data_path, test_data_dir, method):\n",
    "    _, file_name = os.path.split(train_data_path)\n",
    "    dist, num_days, _, num_samples, _, ratio = file_name.replace(\".npy\", \"\").split(\"_\")\n",
    "\n",
    "    # Load data\n",
    "    train_data, test_data_all = load_data(train_data_path, test_data_dir)\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, y_train, X_test_all, y_test_all = preprocess_data(train_data, test_data_all)\n",
    "\n",
    "    # Array for storing results\n",
    "    results = []\n",
    "    num_bins = range(5, 26)\n",
    "    epsilon = 1e-8 # Smoothing hyperparameters\n",
    "\n",
    "    for num_bin in num_bins:\n",
    "        ########################\n",
    "        ### current solution ###\n",
    "        ########################\n",
    "        result = [dist, num_days, num_samples, ratio, num_bin]\n",
    "        print(f\"num_bin = {num_bin}\")\n",
    "\n",
    "        #########################\n",
    "        ### Invoke the solver ###\n",
    "        #########################\n",
    "        start_time = process_time()\n",
    "        if method == \"ewb\":\n",
    "            final_bin_edges = equal_width_cut_points(300, 850, num_bin, np.sum(X_train, axis=0))\n",
    "        elif method == \"efb\":\n",
    "            final_bin_edges = equal_freq_cut_points(300, 850, num_bin, np.sum(X_train, axis=0))\n",
    "        else:\n",
    "            raise Exception(\"Not implemented method\")\n",
    "        # final_bin_edges = equal_width_cut_points_naive(300, 850, num_bin)\n",
    "        end_time = process_time()\n",
    "        solving_time = end_time - start_time\n",
    "        result.append(solving_time)\n",
    "        \n",
    "        # print(f\"Time for solving: {solving_time} s\")\n",
    "        # print(\"final_bin_edges =\", final_bin_edges, \"\\n\")\n",
    "\n",
    "\n",
    "        ###############\n",
    "        ### Evaluation ###\n",
    "        ###############\n",
    "        alphas = np.arange(0, 1.05, 0.05)\n",
    "        alphas = [round(alpha, 2) for alpha in alphas]\n",
    "\n",
    "        psi_0_train, psi_1_train, all_total_objective_train = calculate_objectives(X_train, y_train, final_bin_edges, alphas)\n",
    "        train_objectives = {\"Objective_0\": psi_0_train, \"Objective_1\": psi_1_train, \"Total Objective\": all_total_objective_train}\n",
    "        result.append(train_objectives)\n",
    "\n",
    "        # thresholds = np.arange(0.01, 1.01, 0.01)\n",
    "        # thresholds = [round(threshold, 2) for threshold in thresholds]\n",
    "        thresholds = [0.1]\n",
    "                \n",
    "        # Training Acccuracy & F1 & F2\n",
    "        num_days_train = X_train.shape[0]\n",
    "        best_train_threshold = best_train_precision_0 = best_train_recall_1 = best_train_inverse_weighted_f2 = 0\n",
    "        best_y_train_pred = [0] * (num_days_train - 1)\n",
    "        train_acc = 0\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_train_pred = []\n",
    "            \n",
    "            for i in range(num_days_train - 1):\n",
    "                hist_1 = []\n",
    "                for j in range(len(final_bin_edges) - 1):\n",
    "                    hist_1.append(np.sum(X_train[i, final_bin_edges[j] - 300: final_bin_edges[j + 1] - 300]))\n",
    "                hist_1 = np.array(hist_1)\n",
    "\n",
    "                hist_2 = []\n",
    "                for j in range(len(final_bin_edges) - 1):\n",
    "                    hist_2.append(np.sum(X_train[i + 1, final_bin_edges[j] - 300: final_bin_edges[j + 1] - 300]))\n",
    "                hist_2 = np.array(hist_2)\n",
    "\n",
    "                psis = (hist_1 - hist_2) * np.log((hist_1 + epsilon) / (hist_2 + epsilon))\n",
    "                psi = np.sum(psis)\n",
    "        \n",
    "                if (y_train[i] == 0 and psi < threshold) or (y_train[i] == 1 and psi >= threshold):\n",
    "                    y_train_pred.append(y_train[i])\n",
    "                else:\n",
    "                    y_train_pred.append(1 - y_train[i])\n",
    "            \n",
    "            train_precision_0, train_recall_1, train_inverse_weighted_f2 = precision_0_recall_1_inverse_weighted_fbeta(y_train, y_train_pred, beta=2.0)\n",
    "            if train_inverse_weighted_f2 > best_train_inverse_weighted_f2:\n",
    "                best_train_inverse_weighted_f2 = train_inverse_weighted_f2\n",
    "                best_train_threshold = threshold\n",
    "                best_train_precision_0 = train_precision_0\n",
    "                best_train_recall_1 = train_recall_1\n",
    "                best_y_train_pred = y_train_pred\n",
    "                train_acc = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "        # print(\"Best threshold:\", best_train_threshold)\n",
    "        result.append(best_train_threshold)\n",
    "\n",
    "        # print(\"Training Accuracy:\", train_acc)\n",
    "        result.append(train_acc)\n",
    "\n",
    "        # print(\"Best Training Precision 0:\", best_train_precision_0)\n",
    "        result.append(best_train_precision_0)   \n",
    "\n",
    "        # print(\"Best Training Recall 1:\", best_train_recall_1)\n",
    "        result.append(best_train_recall_1)\n",
    "\n",
    "        # print(\"Best Training Inverse Weighted F2\", best_train_inverse_weighted_f2)\n",
    "        result.append(best_train_inverse_weighted_f2) \n",
    "\n",
    "        # print(confusion_matrix(y_train, best_y_train_pred))\n",
    "                \n",
    "        # Testing Acccuracy & F1 & F2\n",
    "        for i in range(len(X_test_all)):\n",
    "            X_test, y_test = X_test_all[i], y_test_all[i]\n",
    "            num_days_test = X_test.shape[0]\n",
    "            y_test_pred = []\n",
    "\n",
    "            alphas = np.arange(0, 1.05, 0.05)\n",
    "            alphas = [round(alpha, 2) for alpha in alphas]\n",
    "\n",
    "            psi_0_test, psi_1_test, all_total_objective_test = calculate_objectives(X_test, y_test, final_bin_edges, alphas)\n",
    "            test_objectives = {\"Objective_0\": psi_0_test, \"Objective_1\": psi_1_test, \"Total Objective\": all_total_objective_test}\n",
    "            result.append(test_objectives)\n",
    "\n",
    "            for i in range(num_days_test - 1):\n",
    "                hist_1 = []\n",
    "                for j in range(len(final_bin_edges) - 1):\n",
    "                    hist_1.append(np.sum(X_test[i, final_bin_edges[j] - 300: final_bin_edges[j + 1] - 300]))\n",
    "                hist_1 = np.array(hist_1)\n",
    "\n",
    "                hist_2 = []\n",
    "                for j in range(len(final_bin_edges) - 1):\n",
    "                    hist_2.append(np.sum(X_test[i + 1, final_bin_edges[j] - 300: final_bin_edges[j + 1] - 300]))\n",
    "                hist_2 = np.array(hist_2)\n",
    "\n",
    "                psis = (hist_1 - hist_2) * np.log((hist_1 + epsilon) / (hist_2 + epsilon))\n",
    "                psi = np.sum(psis)\n",
    "\n",
    "                if (y_test[i] == 0 and psi < best_train_threshold) or (y_test[i] == 1 and psi >= best_train_threshold):\n",
    "                    y_test_pred.append(y_test[i])\n",
    "                else:\n",
    "                    y_test_pred.append(1 - y_test[i])\n",
    "\n",
    "            test_acc = accuracy_score(y_test, y_test_pred)\n",
    "            # print(\"Testing Accuracy:\", test_acc)\n",
    "            result.append(test_acc)\n",
    "            \n",
    "            test_precision_0, test_recall_1, test_inverse_weighted_f2 = precision_0_recall_1_inverse_weighted_fbeta(y_test, y_test_pred, beta=2.0)\n",
    "\n",
    "            # print(\"Testing Precision 0:\", test_precision_0)\n",
    "            result.append(test_precision_0)   \n",
    "\n",
    "            # print(\"Testing Recall 1:\", test_recall_1)\n",
    "            result.append(test_recall_1)\n",
    "\n",
    "            # print(\"Testing Inverse Weighted F2:\", test_inverse_weighted_f2)\n",
    "            result.append(test_inverse_weighted_f2)\n",
    "\n",
    "            # print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "def save_results(results, test_data_dir):\n",
    "    test_data_paths = glob(f\"{test_data_dir}/*.npy\")\n",
    "    id2file = {}\n",
    "    for i in range(len(test_data_paths)):\n",
    "        test_file = os.path.split(test_data_paths[i])[1].replace(\".npy\", \"\")\n",
    "        id2file[i] = test_file\n",
    "\n",
    "    df_columns = [\"distribution\", \"num_days\", \"num_samples\", \"ratio\", \n",
    "                  \"num_bin\", \"solving_time\", \"train_objectives\", \"best_threshold\", \n",
    "                  \"training_acc\", \"training_precision_0\", \"training_recall_1\", \"training_inverse_weighted_f2\"]\n",
    "\n",
    "    for i in range(len(test_data_paths)):\n",
    "        df_columns.append(f\"{id2file[i]}_objectives\")\n",
    "        df_columns.append(f\"{id2file[i]}_acc\")\n",
    "        df_columns.append(f\"{id2file[i]}_precision_0\")\n",
    "        df_columns.append(f\"{id2file[i]}_recall_1\")\n",
    "        df_columns.append(f\"{id2file[i]}_inverse_weighted_f2\")\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=df_columns)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_data_dir, test_data_dir, method, save_dir):\n",
    "    train_data_paths = glob(f\"{train_data_dir}/*.npy\")\n",
    "    dist = train_data_dir.split(\"/\")[-1]\n",
    "    items = [(train_data_path, test_data_dir, method) for train_data_path in train_data_paths]\n",
    "\n",
    "    total_results_df = pd.DataFrame()\n",
    "\n",
    "    with Pool() as pool:\n",
    "        for results in pool.starmap(solve, items):\n",
    "            results_df = save_results(results, test_data_dir)\n",
    "            total_results_df = pd.concat([total_results_df, results_df])\n",
    "\n",
    "    total_results_df.to_csv(f\"{save_dir}/{method}_{dist}_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bin = 5num_bin = 5\n",
      "\n",
      "num_bin = 5\n",
      "num_bin = 6\n",
      "num_bin = 6\n",
      "num_bin = 6\n",
      "num_bin = 7\n",
      "num_bin = 7\n",
      "num_bin = 7\n",
      "num_bin = 8\n",
      "num_bin = 8\n",
      "num_bin = 8\n",
      "num_bin = 9\n",
      "num_bin = 9\n",
      "num_bin = 9\n",
      "num_bin = 10\n",
      "num_bin = 10\n",
      "num_bin = 10\n",
      "num_bin = 11\n",
      "num_bin = 11\n",
      "num_bin = 11\n",
      "num_bin = 12\n",
      "num_bin = 12\n",
      "num_bin = 12\n",
      "num_bin = 13\n",
      "num_bin = 13\n",
      "num_bin = 13\n",
      "num_bin = 14\n",
      "num_bin = 14\n",
      "num_bin = 14\n",
      "num_bin = 15\n",
      "num_bin = 15\n",
      "num_bin = 15\n",
      "num_bin = 16\n",
      "num_bin = 16\n",
      "num_bin = 16\n",
      "num_bin = 17\n",
      "num_bin = 17\n",
      "num_bin = 17\n",
      "num_bin = 18\n",
      "num_bin = 18\n",
      "num_bin = 18\n",
      "num_bin = 19\n",
      "num_bin = 19\n",
      "num_bin = 19\n",
      "num_bin = 20\n",
      "num_bin = 20\n",
      "num_bin = 20\n",
      "num_bin = 21\n",
      "num_bin = 21\n",
      "num_bin = 21\n",
      "num_bin = 22\n",
      "num_bin = 22\n",
      "num_bin = 22\n",
      "num_bin = 23\n",
      "num_bin = 23\n",
      "num_bin = 23\n",
      "num_bin = 24\n",
      "num_bin = 24\n",
      "num_bin = 24\n",
      "num_bin = 25\n",
      "num_bin = 25\n",
      "num_bin = 25\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = \"../data/test/logistic/plain\"\n",
    "test_data_dir = \"../data/small_psi_data/test-logistic\"\n",
    "save_dir = f\"../output/test\"\n",
    "method = \"ewb\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(train_data_dir, test_data_dir, method, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_discretization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "871ce1bc0dd274fbcbdef4ad0365f378d1ced7d5dc441c7b13130107f9334e06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
