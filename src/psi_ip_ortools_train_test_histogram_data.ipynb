{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import process_time\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "from ortools.linear_solver import pywraplp\n",
    "\n",
    "# settings to display all columns\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metrics\n",
    "def precision_0_recall_1_inverse_weighted_fbeta(y_true, y_pred, beta=2.0):\n",
    "    precisions, recalls, fbeta_scores, supports = precision_recall_fscore_support(y_true, y_pred, beta=beta, average=None)\n",
    "\n",
    "    precision_0 = round(precisions[0], 4)\n",
    "    recall_1 = round(recalls[1], 4)\n",
    "    ratio_0, ratio_1 = supports / sum(supports)\n",
    "    inverse_weighted_fbeta_score = round(fbeta_scores[0]*ratio_1 + fbeta_scores[1]*ratio_0, 4)\n",
    "    \n",
    "    return precision_0, recall_1, inverse_weighted_fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "def load_data(train_data_path, test_data_dir):\n",
    "    # Training data\n",
    "    train_data = np.load(train_data_path)\n",
    "    \n",
    "    # Testing data\n",
    "    test_data_paths = glob(f\"{test_data_dir}/*.npy\")\n",
    "    test_data_all = [np.load(test_data_path) for test_data_path in test_data_paths]\n",
    "        \n",
    "    return train_data, test_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "def preprocess_data(train_data, test_data_all):\n",
    "    # Training features and labels\n",
    "    X_train = train_data[:, :-1]\n",
    "    X_train = X_train / sum(X_train[0])\n",
    "    y_train = train_data[1:, -1]\n",
    "\n",
    "    # Testing features and labels\n",
    "    X_test_all = [test_data[:, :-1] / sum(test_data[0, :-1]) for test_data in test_data_all]\n",
    "    y_test_all = [test_data[1:, -1] for test_data in test_data_all]\n",
    "\n",
    "    return X_train, y_train, X_test_all, y_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(train_data_path, test_data_dir):\n",
    "    _, file_name = os.path.split(train_data_path)\n",
    "    dist, num_days, _, num_samples, _, ratio = file_name.replace(\".npy\", \"\").split(\"_\")\n",
    "\n",
    "    # Load data\n",
    "    train_data, test_data_all = load_data(train_data_path, test_data_dir)\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, y_train, X_test_all, y_test_all = preprocess_data(train_data, test_data_all)\n",
    "\n",
    "    start_time = process_time()\n",
    "\n",
    "    # Preparing interval frequencies\n",
    "    min_edge, max_edge = 300, 850\n",
    "    bin_edges = np.arange(min_edge, max_edge + 1, 1)\n",
    "\n",
    "    train_size = len(bin_edges)\n",
    "    num_days_train = X_train.shape[0]\n",
    "    percent_days_train = np.zeros((num_days_train, train_size, train_size))\n",
    "\n",
    "    for i in range(num_days_train):\n",
    "        hist = X_train[i]\n",
    "        for j in range(train_size - 1):\n",
    "            for k in range(j + 1, train_size):\n",
    "                percent_days_train[i, j, k] = np.sum(hist[j: k])    \n",
    "\n",
    "    # Preparing PSIs\n",
    "    epsilon = 1e-8 # Smoothing hyperparameters\n",
    "\n",
    "    psi_train = []\n",
    "    for i in range(1, num_days_train):\n",
    "        psi_train.append((percent_days_train[i] - percent_days_train[i - 1]) * np.log((percent_days_train[i] + epsilon) / (percent_days_train[i - 1] + epsilon)))\n",
    "    psi_train = np.array(psi_train)\n",
    "\n",
    "    # PSI_0\n",
    "    psi_0_train = psi_train[(1 - y_train).astype(bool)]\n",
    "    psi_0_train = np.sum(psi_0_train, axis=0)\n",
    "    # Normalization\n",
    "    psi_0_train = psi_0_train / np.sum(1 - y_train)\n",
    "\n",
    "\n",
    "    # PSI_1\n",
    "    psi_1_train = psi_train[y_train.astype(bool)]\n",
    "    psi_1_train = np.sum(psi_1_train, axis=0)\n",
    "    # Normalization\n",
    "    psi_1_train = psi_1_train / np.sum(y_train)\n",
    "\n",
    "\n",
    "    end_time = process_time()\n",
    "    preparing_data_time = end_time - start_time\n",
    "\n",
    "    print(f\"Time for preparing data: {preparing_data_time} s\")\n",
    "\n",
    "    # Declare the model\n",
    "    solver = pywraplp.Solver.CreateSolver('SCIP')\n",
    "\n",
    "    # Create the variables\n",
    "    x = np.empty(shape=(train_size, train_size), dtype=object)\n",
    "\n",
    "    for i in range(train_size):\n",
    "        for j in range(train_size):\n",
    "            if j > i:\n",
    "                x[i, j] = solver.IntVar(0, 1, f'x[{i}, {j}]')\n",
    "            else:\n",
    "                x[i, j] = 0\n",
    "\n",
    "    # Create the constraints\n",
    "    start_time = process_time()\n",
    "\n",
    "    # Each row/column has at most one 1\n",
    "    # Non-overlap bins (a.k.a flow constraint)\n",
    "    for i in range(1, train_size - 1):\n",
    "        solver.Add(solver.Sum(x[: i, i]) <= 1)\n",
    "        solver.Add(solver.Sum(x[i, i + 1:]) <= 1)\n",
    "        solver.Add(solver.Sum(x[: i, i]) == solver.Sum(x[i, i + 1:]))\n",
    "        \n",
    "    # Ensure in-and-out\n",
    "    solver.Add(solver.Sum(x[0, 1:]) == 1)\n",
    "    solver.Add(solver.Sum(x[0: -1, -1]) == 1)\n",
    "\n",
    "    # Ensure at most k bins\n",
    "    max_num_bins = 25\n",
    "    min_num_bins = 5\n",
    "    solver.Add(solver.Sum(x.flatten()) <= max_num_bins)\n",
    "    solver.Add(solver.Sum(x.flatten()) >= min_num_bins)\n",
    "\n",
    "    end_time = process_time()\n",
    "    constraints_time = end_time - start_time\n",
    "\n",
    "    print(f\"Time for creating constraints: {constraints_time} s\")\n",
    "\n",
    "    # Solve\n",
    "    # Array fir storing results\n",
    "    results = []\n",
    "\n",
    "    # alphas = np.arange(0, 1.05, 0.05)\n",
    "    # alphas = [round(alpha, 2) for alpha in alphas]\n",
    "    alphas = [0.5]\n",
    "\n",
    "    for alpha in alphas:  \n",
    "        ########################\n",
    "        ### current solution ###\n",
    "        ########################\n",
    "        result = [dist, num_days, num_samples, ratio, alpha, preparing_data_time, constraints_time]\n",
    "        print(f\"alpha = {alpha}\")\n",
    "\n",
    "        \n",
    "        #######################\n",
    "        ### Multi-objective ###\n",
    "        #######################\n",
    "        solver.Maximize(solver.Sum((alpha * psi_1_train * x).flatten()) - solver.Sum(((1 - alpha) * psi_0_train * x).flatten()))\n",
    "        \n",
    "        \n",
    "        #########################\n",
    "        ### Invoke the solver ###\n",
    "        #########################\n",
    "        start_time = process_time()\n",
    "        status = solver.Solve()\n",
    "        end_time = process_time()\n",
    "        solving_time = end_time - start_time\n",
    "        \n",
    "        result.append(solving_time)\n",
    "        print(f\"Time for solving: {solving_time} s\")\n",
    "        \n",
    "        \n",
    "        ##########################\n",
    "        ### Print the solution ###\n",
    "        ##########################\n",
    "        x_solution_value = np.zeros((train_size, train_size))\n",
    "\n",
    "        for i in range(train_size):\n",
    "            for j in range(train_size):\n",
    "                if j > i:\n",
    "                    x_solution_value[i, j] = x[i, j].solution_value()\n",
    "                    \n",
    "        final_bin_edges = []\n",
    "\n",
    "        if status == pywraplp.Solver.OPTIMAL or status == pywraplp.Solver.FEASIBLE:\n",
    "            \n",
    "            objective_0 = np.sum(psi_0_train * x_solution_value)\n",
    "            result.append(objective_0)\n",
    "            print(f\"Objective_0 = {objective_0}\")\n",
    "            \n",
    "            objective_1 = np.sum(psi_1_train * x_solution_value)\n",
    "            result.append(objective_1)\n",
    "            print(f\"Objective_1 = {objective_1}\")\n",
    "\n",
    "            total_cost = solver.Objective().Value()\n",
    "            result.append(total_cost)\n",
    "            print(f\"Total cost = {total_cost}\", \"\\n\")\n",
    "\n",
    "            for i in range(train_size):\n",
    "                for j in range(train_size):\n",
    "                    if j > i and x[i, j].solution_value() == 1:\n",
    "                        final_bin_edges.append(i + 300)\n",
    "            final_bin_edges.append(max_edge)\n",
    "        else:\n",
    "            print('No solution found.')\n",
    "            \n",
    "        final_num_bin = len(final_bin_edges) - 1\n",
    "        print(\"final_bin_edges =\", final_bin_edges, \"\\n\")\n",
    "        result.append(final_num_bin)\n",
    "                \n",
    "        \n",
    "        ###############\n",
    "        ### Evaluation ###\n",
    "        ###############\n",
    "        # thresholds = np.arange(0.01, 1.01, 0.01)\n",
    "        # thresholds = [round(threshold, 2) for threshold in thresholds]\n",
    "        thresholds = [0.1]\n",
    "                \n",
    "        # Training Acccuracy & F1 & F0.5\n",
    "        num_days_train = X_train.shape[0]\n",
    "        best_train_threshold = best_train_precision_0 = best_train_recall_1 = best_train_inverse_weighted_f2 = 0\n",
    "        best_y_train_pred = [0] * (num_days_train - 1)\n",
    "        train_acc = 0\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_train_pred = []\n",
    "            \n",
    "            for i in range(num_days_train - 1):\n",
    "                hist_1 = []\n",
    "                for j in range(len(final_bin_edges) - 1):\n",
    "                    hist_1.append(np.sum(X_train[i, final_bin_edges[j] - 300: final_bin_edges[j + 1] - 300]))\n",
    "                hist_1 = np.array(hist_1)\n",
    "\n",
    "                hist_2 = []\n",
    "                for j in range(len(final_bin_edges) - 1):\n",
    "                    hist_2.append(np.sum(X_train[i + 1, final_bin_edges[j] - 300: final_bin_edges[j + 1] - 300]))\n",
    "                hist_2 = np.array(hist_2)\n",
    "\n",
    "                psis = (hist_1 - hist_2) * np.log((hist_1 + epsilon) / (hist_2 + epsilon))\n",
    "                psi = np.sum(psis)\n",
    "        \n",
    "                if (y_train[i] == 0 and psi < threshold) or (y_train[i] == 1 and psi >= threshold):\n",
    "                    y_train_pred.append(y_train[i])\n",
    "                else:\n",
    "                    y_train_pred.append(1 - y_train[i])\n",
    "            \n",
    "            train_precision_0, train_recall_1, train_inverse_weighted_f2 = precision_0_recall_1_inverse_weighted_fbeta(y_train, y_train_pred, beta=2.0)\n",
    "            if train_inverse_weighted_f2 > best_train_inverse_weighted_f2:\n",
    "                best_train_inverse_weighted_f2 = train_inverse_weighted_f2\n",
    "                best_train_threshold = threshold\n",
    "                best_train_precision_0 = train_precision_0\n",
    "                best_train_recall_1 = train_recall_1\n",
    "                best_y_train_pred = y_train_pred\n",
    "                train_acc = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "        print(\"Best threshold:\", best_train_threshold)\n",
    "        result.append(best_train_threshold)\n",
    "\n",
    "        print(\"Training Accuracy:\", train_acc)\n",
    "        result.append(train_acc)\n",
    "\n",
    "        print(\"Best Training Precision 0:\", best_train_precision_0)\n",
    "        result.append(best_train_precision_0)   \n",
    "\n",
    "        print(\"Best Training Recall 1:\", best_train_recall_1)\n",
    "        result.append(best_train_recall_1)\n",
    "\n",
    "        print(\"Best Training Inverse Weighted F2\", best_train_inverse_weighted_f2)\n",
    "        result.append(best_train_inverse_weighted_f2) \n",
    "\n",
    "        print(confusion_matrix(y_train, best_y_train_pred))\n",
    "                \n",
    "        # Testing Acccuracy & F1 & F2\n",
    "        for i in range(len(X_test_all)):\n",
    "            X_test, y_test = X_test_all[i], y_test_all[i]\n",
    "            num_days_test = X_test.shape[0]\n",
    "            y_test_pred = []\n",
    "            psi_0_test = psi_1_test = 0\n",
    "\n",
    "            for i in range(num_days_test - 1):\n",
    "                hist_1 = []\n",
    "                for j in range(len(final_bin_edges) - 1):\n",
    "                    hist_1.append(np.sum(X_test[i, final_bin_edges[j] - 300: final_bin_edges[j + 1] - 300]))\n",
    "                hist_1 = np.array(hist_1)\n",
    "\n",
    "                hist_2 = []\n",
    "                for j in range(len(final_bin_edges) - 1):\n",
    "                    hist_2.append(np.sum(X_test[i + 1, final_bin_edges[j] - 300: final_bin_edges[j + 1] - 300]))\n",
    "                hist_2 = np.array(hist_2)\n",
    "\n",
    "                psis = (hist_1 - hist_2) * np.log((hist_1 + epsilon) / (hist_2 + epsilon))\n",
    "                psi = np.sum(psis)\n",
    "\n",
    "                if y_test[i] == 0:\n",
    "                    psi_0_test += psi\n",
    "                else:\n",
    "                    psi_1_test += psi\n",
    "\n",
    "                if (y_test[i] == 0 and psi < best_train_threshold) or (y_test[i] == 1 and psi >= best_train_threshold):\n",
    "                    y_test_pred.append(y_test[i])\n",
    "                else:\n",
    "                    y_test_pred.append(1 - y_test[i])\n",
    "\n",
    "            psi_0_test = psi_0_test / np.sum(1 - y_test)\n",
    "            print(\"Testing Objective_0:\", psi_0_test)\n",
    "            result.append(psi_0_test)\n",
    "\n",
    "            psi_1_test = psi_1_test / np.sum(y_test)\n",
    "            print(\"Testing Objective_1:\", psi_1_test)\n",
    "            result.append(psi_1_test)\n",
    "\n",
    "            total_objective_test = alpha*psi_1_test - (1 - alpha)*psi_0_test\n",
    "            print(\"Testing Total Objective:\", total_objective_test)\n",
    "            result.append(total_objective_test)\n",
    "\n",
    "            test_acc = accuracy_score(y_test, y_test_pred)\n",
    "            print(\"Testing Accuracy:\", test_acc)\n",
    "            result.append(test_acc)\n",
    "            \n",
    "            test_precision_0, test_recall_1, test_inverse_weighted_f2 = precision_0_recall_1_inverse_weighted_fbeta(y_test, y_test_pred, beta=2.0)\n",
    "\n",
    "            print(\"Testing Precision 0:\", test_precision_0)\n",
    "            result.append(test_precision_0)   \n",
    "\n",
    "            print(\"Testing Recall 1:\", test_recall_1)\n",
    "            result.append(test_recall_1)\n",
    "\n",
    "            print(\"Testing Inverse Weighted F2:\", test_inverse_weighted_f2)\n",
    "            result.append(test_inverse_weighted_f2)\n",
    "\n",
    "            print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "def save_results(results, test_data_dir):\n",
    "    test_data_paths = glob(f\"{test_data_dir}/*.npy\")\n",
    "    id2file = {}\n",
    "    for i in range(len(test_data_paths)):\n",
    "        test_file = os.path.split(test_data_paths[i])[1].replace(\".npy\", \"\")\n",
    "        id2file[i] = test_file\n",
    "\n",
    "    df_columns = [\"distribution\", \"num_days\", \"num_samples\", \"ratio\", \"alpha\", \n",
    "                \"preparing_data_time\", \"creating_constraints_time\", \"solving_time\", \n",
    "                \"objective_0\", \"objective_1\", \"total_cost\", \"final_num_bin\", \"best_threshold\",\n",
    "                \"training_acc\", \"training_precision_0\", \"training_recall_1\", \"training_inverse_weighted_f2\"]\n",
    "\n",
    "    for i in range(len(test_data_paths)):\n",
    "        df_columns.append(f\"{id2file[i]}_objective_0\")\n",
    "        df_columns.append(f\"{id2file[i]}_objective_1\")\n",
    "        df_columns.append(f\"{id2file[i]}_total_objective\")\n",
    "        df_columns.append(f\"{id2file[i]}_acc\")\n",
    "        df_columns.append(f\"{id2file[i]}_precision_0\")\n",
    "        df_columns.append(f\"{id2file[i]}_recall_1\")\n",
    "        df_columns.append(f\"{id2file[i]}_inverse_weighted_f2\")\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=df_columns)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_data_list, test_data_dir, save_dir):\n",
    "    items = [(train_data_path, test_data_dir) for train_data_path in train_data_list]\n",
    "    total_results_df = pd.DataFrame()\n",
    "\n",
    "    with Pool() as pool:\n",
    "        for results in pool.starmap(solve, items):\n",
    "            results_df = save_results(results, test_data_dir)\n",
    "            total_results_df = pd.concat([total_results_df, results_df])\n",
    "\n",
    "        total_results_df.to_csv(f\"{save_dir}/test_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for preparing data: 61.021344375999995 s\n",
      "Time for creating constraints: 7.278982006999996 s\n",
      "alpha = 0.5\n",
      "Time for solving: 860.4780620370001 s\n",
      "Objective_0 = 0.020988372434621056\n",
      "Objective_1 = 0.3858336783104368\n",
      "Total cost = 0.18242265293790788 \n",
      "\n",
      "final_bin_edges = [300, 500, 516, 520, 529, 530, 536, 541, 542, 546, 547, 552, 557, 558, 560, 565, 569, 610, 611, 653, 682, 757, 763, 766, 789, 850] \n",
      "\n",
      "Best threshold: 0.1\n",
      "Training Accuracy: 1.0\n",
      "Best Training Precision 0: 1.0\n",
      "Best Training Recall 1: 1.0\n",
      "Best Training Inverse Weighted F2 1.0\n",
      "[[22  0]\n",
      " [ 0  7]]\n",
      "Testing Objective_0: 0.02098837243462106\n",
      "Testing Objective_1: 0.38583367831043686\n",
      "Testing Total Objective: 0.1824226529379079\n",
      "Testing Accuracy: 1.0\n",
      "Testing Precision 0: 1.0\n",
      "Testing Recall 1: 1.0\n",
      "Testing Inverse Weighted F2: 1.0\n",
      "[[22  0]\n",
      " [ 0  7]]\n"
     ]
    }
   ],
   "source": [
    "train_data_list = [\"../data/small_psi_data/logistic/logistic_30_days_1000_samples_70.npy\"]\n",
    "test_data_dir = \"../data/test/logistic/plain\"\n",
    "save_dir = f\"../output/test\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(train_data_list, test_data_dir, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_discretization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "871ce1bc0dd274fbcbdef4ad0365f378d1ced7d5dc441c7b13130107f9334e06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
