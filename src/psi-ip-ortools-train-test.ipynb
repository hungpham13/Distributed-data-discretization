{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Loading data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport numpy as np\nimport pandas as pd\nfrom time import process_time\nfrom glob import glob",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-06T10:21:42.355706Z",
     "iopub.execute_input": "2023-02-06T10:21:42.356554Z",
     "iopub.status.idle": "2023-02-06T10:21:42.388522Z",
     "shell.execute_reply.started": "2023-02-06T10:21:42.356427Z",
     "shell.execute_reply": "2023-02-06T10:21:42.387182Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Training data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train_data_path = \"/kaggle/input/normal-psi/logistic-new/logistic_30_days_1000_samples.npy\"\ntrain_data = np.load(train_data_path)\ntrain_data.shape",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-06T10:26:03.947105Z",
     "iopub.execute_input": "2023-02-06T10:26:03.947438Z",
     "iopub.status.idle": "2023-02-06T10:26:03.977561Z",
     "shell.execute_reply.started": "2023-02-06T10:26:03.947414Z",
     "shell.execute_reply": "2023-02-06T10:26:03.976369Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "execution_count": 2,
     "output_type": "execute_result",
     "data": {
      "text/plain": "(30, 552)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "train_data",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-06T10:26:09.307806Z",
     "iopub.execute_input": "2023-02-06T10:26:09.308161Z",
     "iopub.status.idle": "2023-02-06T10:26:09.317527Z",
     "shell.execute_reply.started": "2023-02-06T10:26:09.308130Z",
     "shell.execute_reply": "2023-02-06T10:26:09.316178Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "execution_count": 3,
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Testing data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "test_data_dir = \"/kaggle/input/test-psi/test-logistic\"",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-06T10:26:14.437834Z",
     "iopub.execute_input": "2023-02-06T10:26:14.438202Z",
     "iopub.status.idle": "2023-02-06T10:26:14.443449Z",
     "shell.execute_reply.started": "2023-02-06T10:26:14.438172Z",
     "shell.execute_reply": "2023-02-06T10:26:14.442221Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test_data_paths = glob(f\"{test_data_dir}/*.npy\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-06T10:26:16.677777Z",
     "iopub.execute_input": "2023-02-06T10:26:16.678176Z",
     "iopub.status.idle": "2023-02-06T10:26:16.717217Z",
     "shell.execute_reply.started": "2023-02-06T10:26:16.678142Z",
     "shell.execute_reply": "2023-02-06T10:26:16.715915Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "id2file = {}\nfor i in range(len(test_data_paths)):\n    test_file = os.path.split(test_data_paths[i])[1].replace(\".npy\", \"\")\n    id2file[i] = test_file\nlen(id2file)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-06T10:26:18.427833Z",
     "iopub.execute_input": "2023-02-06T10:26:18.428234Z",
     "iopub.status.idle": "2023-02-06T10:26:18.437927Z",
     "shell.execute_reply.started": "2023-02-06T10:26:18.428201Z",
     "shell.execute_reply": "2023-02-06T10:26:18.436679Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "execution_count": 6,
     "output_type": "execute_result",
     "data": {
      "text/plain": "45"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "test_data_all = [np.load(test_data_path) for test_data_path in test_data_paths]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-06T10:26:21.601823Z",
     "iopub.execute_input": "2023-02-06T10:26:21.602200Z",
     "iopub.status.idle": "2023-02-06T10:26:23.180707Z",
     "shell.execute_reply.started": "2023-02-06T10:26:21.602169Z",
     "shell.execute_reply": "2023-02-06T10:26:23.179306Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test_data_all[0].shape",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-06T10:26:23.878539Z",
     "iopub.execute_input": "2023-02-06T10:26:23.880196Z",
     "iopub.status.idle": "2023-02-06T10:26:23.887741Z",
     "shell.execute_reply.started": "2023-02-06T10:26:23.880142Z",
     "shell.execute_reply": "2023-02-06T10:26:23.886024Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1095, 552)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Training data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "X_train = train_data[:, :-1]\nX_train.shape",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "y_train = train_data[:, -1]\ny_train.shape",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(f\"0: {len(y_train[y_train == 0])}\")\nprint(f\"1: {len(y_train[y_train == 1])}\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Testing data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "X_test_all = [test_data[:, :-1] for test_data in test_data_all]\nfor i in range(len(X_test_all)):\n    X_test_all[i] = X_test_all[i] / np.sum(X_test_all[i][0])\nprint(len(X_test_all))\nprint(X_test_all[0].shape)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "y_test_all = [test_data[1:, -1] for test_data in test_data_all]\ny_test_all[0].shape",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Preparing data for training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "start_time = process_time()\n\n\n# Preparing interval frequencies\nmin_edge, max_edge = 300, 850\nbin_edges = np.arange(min_edge, max_edge + 1, 1)\n\ntrain_size = len(bin_edges)\nnum_days_train = X_train.shape[0]\npercent_days_train = np.zeros((num_days_train, train_size, train_size))\n\nfor i in range(num_days_train):\n    hist, _ = np.histogram(X_train[i], bins=bin_edges, density=True)\n    for j in range(train_size - 1):\n        for k in range(j + 1, train_size):\n            percent_days_train[i, j, k] = np.sum(hist[j: k])\n            \n\n# Preparing PSIs\nepsilon = 1e-8 # Smoothing hyperparameters\n\npsi_train = []\nfor i in range(1, num_days_train):\n    psi_train.append((percent_days_train[i] - percent_days_train[i - 1]) * np.log((percent_days_train[i] + epsilon) / (percent_days_train[i - 1] + epsilon)))\npsi_train = np.array(psi_train)\n\n# PSI_0\npsi_0_train = psi_train[(1 - y_train).astype(bool)]\npsi_0_train = np.sum(psi_0_train, axis=0)\n# Normalization\npsi_0_train = psi_0_train / np.sum(1 - y_train)\n\n\n# PSI_1\npsi_1_train = psi_train[y_train.astype(bool)]\npsi_1_train = np.sum(psi_1_train, axis=0)\n# Normalization\npsi_1_train = psi_1_train / np.sum(y_train)\n\n\nend_time = process_time()\npreparing_data_time = end_time - start_time\n\nprint(f\"Time for preparing data: {preparing_data_time} s\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Models",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from ortools.linear_solver import pywraplp",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Declare the model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "solver = pywraplp.Solver.CreateSolver('SCIP')",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Create the variables",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "x = np.empty(shape=(train_size, train_size), dtype=object)\n\nfor i in range(train_size):\n    for j in range(train_size):\n        if j > i:\n            x[i, j] = solver.IntVar(0, 1, f'x[{i}, {j}]')\n        else:\n            x[i, j] = 0",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Create the constraints",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "start_time = process_time()\n\n# Each row/column has at most one 1\n# Non-overlap bins (a.k.a flow constraint)\nfor i in range(1, train_size - 1):\n    solver.Add(solver.Sum(x[: i, i]) <= 1)\n    solver.Add(solver.Sum(x[i, i + 1:]) <= 1)\n    solver.Add(solver.Sum(x[: i, i]) == solver.Sum(x[i, i + 1:]))\n    \n# Ensure in-and-out\nsolver.Add(solver.Sum(x[0, 1:]) == 1)\nsolver.Add(solver.Sum(x[0: -1, -1]) == 1)\n\n# Ensure at most k bins\nmax_num_bins = 25\nmin_num_bins = 5\nsolver.Add(solver.Sum(x.flatten()) <= max_num_bins)\nsolver.Add(solver.Sum(x.flatten()) >= min_num_bins)\n\nend_time = process_time()\nconstraints_time = end_time - start_time\n\nprint(f\"Time for creating constraints: {constraints_time} s\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Create the objective function & Invoke the solver & Print the solution & Testing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import accuracy_score, f1_score, fbeta_score, confusion_matrix, classification_report",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_dir_path, file_name = os.path.split(train_data_path)\ndist, num_days, _, num_samples, _ = file_name.split(\"_\")\n\nprint(dist)\nprint(num_days)\nprint(num_samples)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Array fir storing results\nresults = []\n\nalphas = np.arange(0, 1.05, 0.05)\nalphas = [round(alpha, 2) for alpha in alphas]\n# alphas = [0.5]\n\nfor alpha in alphas:  \n    ########################\n    ### current solution ###\n    ########################\n    result = [dist, num_days, num_samples, alpha, preparing_data_time, constraints_time]\n    print(f\"alpha = {alpha}\")\n\n    \n    #######################\n    ### Multi-objective ###\n    #######################\n    solver.Maximize(solver.Sum((alpha * psi_1_train * x).flatten()) - solver.Sum(((1 - alpha) * psi_0_train * x).flatten()))\n    \n    \n    #########################\n    ### Invoke the solver ###\n    #########################\n    start_time = process_time()\n    status = solver.Solve()\n    end_time = process_time()\n    solving_time = end_time - start_time\n    \n    result.append(solving_time)\n    print(f\"Time for solving: {solving_time} s\")\n    \n    \n    ##########################\n    ### Print the solution ###\n    ##########################\n    x_solution_value = np.zeros((train_size, train_size))\n\n    for i in range(train_size):\n        for j in range(train_size):\n            if j > i:\n                x_solution_value[i, j] = x[i, j].solution_value()\n                \n    final_bin_edges = []\n\n    if status == pywraplp.Solver.OPTIMAL or status == pywraplp.Solver.FEASIBLE:\n        total_cost = solver.Objective().Value()\n        result.append(total_cost)\n        print(f\"Total cost = {total_cost}\")\n        \n        objective_0 = np.sum(psi_0_train * x_solution_value)\n        result.append(objective_0)\n        print(f\"Objective_0 = {objective_0}\")\n        \n        objective_1 = np.sum(psi_1_train * x_solution_value)\n        result.append(objective_1)\n        print(f\"Objective_1 = {objective_1}\", \"\\n\")\n\n        for i in range(train_size):\n            for j in range(train_size):\n                if j > i and x[i, j].solution_value() == 1:\n                    final_bin_edges.append(i + 300)\n        final_bin_edges.append(max_edge)\n    else:\n        print('No solution found.')\n        \n    print(\"final_bin_edges =\", final_bin_edges, \"\\n\")\n              \n    \n    ###############\n    ### Evaluation ###\n    ###############\n    thresholds = np.arange(0.01, 1.01, 0.01)\n    thresholds = [round(threshold, 2) for threshold in thresholds]\n              \n    # Training Acccuracy & F1\n    num_days_train = X_train.shape[0]\n    y_train_pred_all = np.zeros((len(thresholds), num_days_train - 1))\n    train_acc_all = []\n    train_f1_all = []\n    train_f2_all = []\n\n    for i in range(num_days_train - 1):\n        hist_1, _ = np.histogram(X_train[i], bins=final_bin_edges, density=True)\n        hist_2, _ = np.histogram(X_train[i + 1], bins=final_bin_edges, density=True)\n\n        psis = (hist_1 - hist_2) * np.log((hist_1 + epsilon) / (hist_2 + epsilon))\n        psi = np.sum(psis)\n\n        y_train_pred_i = []\n        for threshold in thresholds:        \n            if (y_train[i] == 0 and psi < threshold) or (y_train[i] == 1 and psi > threshold):\n                y_train_pred_i.append(y_train[i])\n            else:\n                y_train_pred_i.append(1 - y_train[i])\n\n        y_train_pred_all[:, i] = y_train_pred_i\n\n    for i in range(len(thresholds)):\n        train_acc = accuracy_score(y_train, y_train_pred_all[i])\n        train_acc_all.append(train_acc)\n\n        train_f1 = f1_score(y_train, y_train_pred_all[i])\n        train_f1_all.append(train_f1)\n\n        train_f2 = fbeta_score(y_train, y_train_pred_all[i], beta=2.0)\n        train_f2_all.append(train_f2)\n        \n    # Best Training F2 index\n    idx = np.argmax(train_f2_all)\n    best_threshold = thresholds[idx]\n    \n    print(\"Best threshold:\", best_threshold)\n    result.append(best_threshold)\n    \n    print(\"Best Training Accuracy:\", train_acc_all[idx])\n    result.append(train_acc_all[idx])\n    \n    print(\"Best Training F1\", train_f1_all[idx])\n    result.append(train_f1_all[idx])\n    \n    print(\"Best Training F2\", train_f2_all[idx])\n    result.append(train_f2_all[idx])    \n              \n    print(confusion_matrix(y_train, y_train_pred_all[idx]))\n    print(classification_report(y_train, y_train_pred_all[idx]))\n    print()\n              \n    # Testing Acccuracy & F1\n    for i in range(len(test_data_paths)):\n        X_test, y_test = X_test_all[i], y_test_all[i]\n        num_days_test = X_test.shape[0]\n        y_test_pred = []\n\n        for i in range(num_days_test - 1):\n            hist_1 = []\n            for j in range(len(final_bin_edges) - 1):\n                hist_1.append(np.sum(X_test[i, final_bin_edges[j]: final_bin_edges[j + 1]]))\n            hist_1[-1] += X_test[i, -1]\n            hist_1 = np.array(hist_1)\n\n            hist_2 = []\n            for j in range(len(final_bin_edges) - 1):\n                hist_2.append(np.sum(X_test[i + 1, final_bin_edges[j]: final_bin_edges[j + 1]]))\n            hist_2[-1] += X_test[i + 1, -1]\n            hist_2 = np.array(hist_2)\n\n            psis = (hist_1 - hist_2) * np.log((hist_1 + epsilon) / (hist_2 + epsilon))\n            psi = np.sum(psis)\n\n            if (y_test[i] == 0 and psi < threshold) or (y_test[i] == 1 and psi > threshold):\n                y_test_pred.append(y_test[i])\n            else:\n                y_test_pred.append(1 - y_test[i])\n\n        test_acc = accuracy_score(y_test, y_test_pred)\n        result.append(test_acc)\n        print(\"Testing Accuracy:\", test_acc)\n\n        test_f1 = f1_score(y_test, y_test_pred)\n        result.append(test_f1)\n        print(\"Testing F1:\", test_f1)\n        \n        test_f2 = fbeta_score(y_test, y_test_pred, beta=2.0)\n        result.append(test_f2)\n        print(\"Testing F2:\", test_f2)\n\n        print(confusion_matrix(y_test, y_test_pred))\n        print(classification_report(y_test, y_test_pred))\n        print(\"\\n\", \"#\"*30, \"\\n\")\n\n    results.append(result)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Saving the results",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "results_df = pd.DataFrame(results)\nresults_df",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df_columns = [\"distribution\", \"num_days\", \"num_samples\", \"alpha\", \n           \"preparing_data_time\", \"creating_constraints_time\", \"solving_time\", \n           \"total_cost\", \"objective_0\", \"objective_1\", \"best_threshold\",\n          \"training_acc\", \"training_f1\", \"training_f2\"]\n\nfor i in range(len(test_data_paths)):\n    df_columns.append(f\"{id2file[i]}_acc\")\n    df_columns.append(f\"{id2file[i]}_f1\")\n    df_columns.append(f\"{id2file[i]}_f2\")\n    \nlen(df_columns)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "results_df.columns = df_columns\nresults_df",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "results_df.to_csv(\"/kaggle/working/results.csv\", index=False)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
